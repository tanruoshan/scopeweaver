import streamlit as st
import pandas as pd
import json
import plotly.express as px
import plotly.graph_objects as go
import os
import datetime
import re

# ==========================================
# 1. PAGE CONFIGURATION & GLOBAL STYLES
# ==========================================
# Sets the browser tab title, layout mode (Wide for dashboards), and favicon.
st.set_page_config(
    page_title="ScopeWeaver Lab",
    layout="wide",
    page_icon="üß™",
    initial_sidebar_state="expanded"
)

# Global Font Configuration for Plotly Charts
# MODIFY HERE to change the font family, size, or color across ALL graphs.
font_config = dict(
    family="Arial, sans-serif", 
    size=14,  # Increased font size for readability
    color="#333"
)

# ==========================================
# 2. DATA INGESTION LAYER
# ==========================================
def load_data():
    """
    Loads the 'results_deep_dive.json' file generated by main.py.
    Returns:
        data (list): List of test result objects.
        timestamp (str): Formatted time of the last file modification.
    """
    try:
        # Get last modified time to show data freshness
        mod_time = os.path.getmtime("results_deep_dive.json")
        timestamp = datetime.datetime.fromtimestamp(mod_time).strftime('%H:%M:%S')
        
        with open("results_deep_dive.json", "r", encoding='utf-8') as f:
            data = json.load(f)
        return data, timestamp
    except FileNotFoundError:
        return [], None

# Load data immediately on script run
raw_data, last_updated = load_data()

# ==========================================
# 3. DATA PRE-PROCESSING & TRANSFORMATION
# ==========================================
if not raw_data:
    st.error("üö® No results found! Run 'main.py' to generate data.")
    st.stop()

# Flatten the nested JSON structure into a Pandas DataFrame for analysis.
rows = []
for d in raw_data:
    errs = d.get('errors', {})
    checks = errs.get('checks', {})
    output_len = len(d.get('raw_output', ''))
    
    # --- LOGIC FOR CONFUSION MATRIX (TOOL & ERROR ID) ---
    # We attempt to extract the "Actual Tool Used" vs "Expected Tool".
    actual_tool = "Parse Error"
    expected_tool = "Unknown"
    
    # 1. Regex Extraction: Find the JSON block in the raw LLM output
    try:
        raw = d.get('raw_output', '')
        match = re.search(r'\{.*\}', raw, re.DOTALL)
        if match:
            obj = json.loads(match.group(0))
            # Extract function name if schema exists, else flag as Schema Error
            actual_tool = obj.get('systemCall', {}).get('Function used', 'Schema Error')
        else:
            actual_tool = "No JSON"
    except:
        actual_tool = "Crash"

    # 2. Determine Expected Tool Logic
    if d.get('passed'):
        expected_tool = actual_tool # If passed, the tool used was correct
    elif 'function' in errs.get('diff_log', {}):
        # If the validator flagged a mismatch, extract the expected value
        diff = errs['diff_log']['function']
        if isinstance(diff, dict):
            expected_tool = diff.get('expected', 'Unknown')
            actual_tool = diff.get('actual', actual_tool)

    # 3. Determine Primary Error Reason (For Heatmaps)
    error_reason = "None"
    if not d.get('passed'):
        if not errs.get('is_valid_json'): error_reason = "Invalid JSON"
        elif not checks.get('no_hallucination'): error_reason = "Hallucination"
        elif not checks.get('function_match'): error_reason = "Wrong Tool"
        elif not checks.get('param_match'): error_reason = "Wrong Param"
        elif not checks.get('error_code_match', True): error_reason = "Wrong Error ID"

    # Append flattened row
    rows.append({
        "id": d.get("id"),
        "category": d.get("category", "N/A"),
        "type": d.get("type", "N/A"),
        "rank": d.get("rank", "N/A"),
        "status": "PASS" if d.get("passed") else "FAIL",
        "output_length": output_len,
        "actual_tool": actual_tool,
        "expected_tool": expected_tool,
        "primary_error": error_reason,
        
        # Binary Signals (1=Pass, 0=Fail) for statistical aggregation
        "check_json": 1 if errs.get('is_valid_json') else 0,
        "check_schema": 1 if errs.get('is_valid_schema') else 0,
        "check_hallucination": 1 if checks.get('no_hallucination') else 0,
        "check_function": 1 if checks.get('function_match') else 0,
        "check_param": 1 if checks.get('param_match') else 0,
        
        "full_record": d # Keep original object for deep-dive inspector
    })

df = pd.DataFrame(rows)

# ==========================================
# 4. SIDEBAR CONTROLS & FILTERS
# ==========================================
st.sidebar.title("üß™ ScopeWeaver Lab")
if last_updated:
    st.sidebar.caption(f"Last Run: {last_updated}")

if st.sidebar.button("üîÑ Force Refresh"):
    st.rerun()

st.sidebar.markdown("---")
st.sidebar.subheader("Data Slicing")

# Multi-select filters allowing users to drill down by Category or Status
f_status = st.sidebar.multiselect("Status", ["PASS", "FAIL"], default=["PASS", "FAIL"])
f_cat = st.sidebar.multiselect("Category", df['category'].unique(), default=df['category'].unique())

# Apply Filters to the Main DataFrame
filtered_df = df[
    (df['status'].isin(f_status)) & 
    (df['category'].isin(f_cat))
]

# ==========================================
# 5. KEY PERFORMANCE INDICATORS (KPIs)
# ==========================================
st.title("üìä Test Suite Analytics")
total = len(filtered_df)
passed = len(filtered_df[filtered_df['status'] == "PASS"])
acc = (passed / total * 100) if total > 0 else 0

# Four-column layout for high-level metrics
c1, c2, c3, c4 = st.columns(4)
c1.metric("Total Samples", total)
c2.metric("Success Rate", f"{acc:.1f}%")
c3.metric("Failures", total - passed, delta_color="inverse")
c4.metric("Avg Verbosity", f"{int(filtered_df['output_length'].mean())} chars")

st.markdown("---")

# ==========================================
# 6. VISUALIZATION TABS
# ==========================================
tab_flow, tab_comp, tab_heat, tab_conf, tab_hier = st.tabs([
    "üåä Failure Flow", 
    "üß© Component Analysis", 
    "üî• Error Heatmap",
    "üòµ Confusion Matrix",
    "‚òÄÔ∏è Hierarchy"
])

# --------------------------------------------------------
# TAB 1: SANKEY DIAGRAM (The "Pipeline of Failure")
# Purpose: Visualizes the drop-off rate at each validation stage.
# --------------------------------------------------------
with tab_flow:
    st.markdown("##### The Pipeline of Failure")
    if not filtered_df.empty:
        total_c = len(filtered_df)
        
        # Calculate drop-off counts for each stage
        valid_json = filtered_df['check_json'].sum(); inv_json = total_c - valid_json
        
        df_j = filtered_df[filtered_df['check_json']==1]
        valid_sch = df_j['check_schema'].sum(); inv_sch = len(df_j) - valid_sch
        
        df_s = df_j[df_j['check_schema']==1]
        no_hal = df_s['check_hallucination'].sum(); hal = len(df_s) - no_hal
        
        df_h = df_s[df_s['check_hallucination']==1]
        val_func = df_h['check_function'].sum(); inv_func = len(df_h) - val_func
        
        df_f = df_h[df_h['check_function']==1]
        val_p = df_f['check_param'].sum(); inv_p = len(df_f) - val_p
        
        # Node Definitions
        labels = ["Start", "Bad JSON", "JSON OK", "Bad Schema", "Schema OK", "Hallucination", "Tool Valid", "Wrong Tool", "Tool OK", "Wrong Param", "PERFECT"]
        
        # Node Colors: Green (#2E8B57) for Success Path, Red (#EF553B) for Failure Nodes
        n_colors = ["#333", "#EF553B", "#2E8B57", "#EF553B", "#2E8B57", "#EF553B", "#2E8B57", "#EF553B", "#2E8B57", "#EF553B", "#00CC96"]

        # Link Colors: Light Green (#98FB98) for Flow, Light Red (#ffcccc) for Drops
        l_colors = ["#ffcccc", "#98FB98", "#ffcccc", "#98FB98", "#ffcccc", "#98FB98", "#ffcccc", "#98FB98", "#ffcccc", "#32CD32"]

        fig_sankey = go.Figure(data=[go.Sankey(
            node = dict(pad=15, thickness=20, line=dict(color="black", width=0.5), label=labels, color=n_colors),
            link = dict(
                source = [0, 0, 2, 2, 4, 4, 6, 6, 8, 8], # Index of Source Node
                target = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], # Index of Target Node
                value  = [inv_json, valid_json, inv_sch, valid_sch, hal, no_hal, inv_func, val_func, inv_p, val_p],
                color  = l_colors
            )
        )])
        fig_sankey.update_layout(font=font_config, height=500)
        st.plotly_chart(fig_sankey, use_container_width=True)

# --------------------------------------------------------
# TAB 2: COMPONENT BARS (Subcategory Analysis)
# Purpose: Compares success rates across Category/Type/Rank.
# --------------------------------------------------------
with tab_comp:
    c_head, c_opt1, c_opt2 = st.columns([2, 1, 1])
    with c_head: st.markdown("##### Subcategory Breakdown")
    with c_opt1: group_by = st.selectbox("Group By:", ["category", "type", "rank"])
    with c_opt2: mode = st.radio("Bars:", ["Grouped", "Stacked"], horizontal=True)

    cols = ['check_json', 'check_hallucination', 'check_function', 'check_param']
    grouped = filtered_df.groupby(group_by)[cols].mean().reset_index()
    melted = grouped.melt(id_vars=group_by, var_name='Check', value_name='Rate')
    melted['Rate'] = melted['Rate'] * 100
    
    # Rename for clearer Legend
    melted['Check'] = melted['Check'].replace({
        'check_json': '1. JSON', 'check_hallucination': '2. Hallucination',
        'check_function': '3. Function', 'check_param': '4. Parameter'
    })
    
    fig_bar = px.bar(melted, x=group_by, y='Rate', color='Check', 
                     barmode="group" if mode=="Grouped" else "stack",
                     range_y=[0, 105] if mode=="Grouped" else None,
                     color_discrete_sequence=px.colors.sequential.Teal) # Teal Color Scale
    fig_bar.update_layout(font=font_config)
    st.plotly_chart(fig_bar, use_container_width=True)

# --------------------------------------------------------
# TAB 3: HEATMAP (Failure Intensity)
# Purpose: Identifies "Hotspots" of failure (e.g., Hard Tests failing Hallucinations).
# --------------------------------------------------------
with tab_heat:
    st.markdown("##### Failure Intensity")
    # Group by category and calculate mean success, then invert for Failure Rate
    heat_grp = filtered_df.groupby('category')[cols].mean()
    heat_data = (1 - heat_grp) * 100 
    
    fig_heat = px.imshow(heat_data, 
                         labels=dict(x="Check Type", y="Category", color="Fail %"),
                         x=['JSON', 'Hallucination', 'Function', 'Param'],
                         color_continuous_scale="Reds", text_auto=".0f")
    fig_heat.update_layout(font=font_config)
    st.plotly_chart(fig_heat, use_container_width=True)

# --------------------------------------------------------
# TAB 4: CONFUSION MATRIX (Tool Usage & Error Types)
# Purpose: Shows what the model used vs what was expected.
# --------------------------------------------------------
with tab_conf:
    c_h, c_o = st.columns([3, 1])
    with c_h: st.markdown("##### Confusion Analysis")
    with c_o: conf_mode = st.radio("Analyze:", ["Tool Usage", "Error Type"])
    
    if conf_mode == "Tool Usage":
        # Filters out Unknowns for cleaner graph
        conf_df = filtered_df[filtered_df['expected_tool'] != "Unknown"]
        x_col, y_col = "actual_tool", "expected_tool"
        title = "Expected Tool (Y) vs Actual Tool (X)"
        # NOTE: histnorm=None ensures ABSOLUTE VALUES (Counts), not percentages.
        normalization = None 
    else:
        # Analyzes why specific categories are failing
        conf_df = filtered_df[filtered_df['status'] == "FAIL"]
        x_col, y_col = "primary_error", "category"
        title = "Category (Y) vs Primary Cause of Failure (X)"
        normalization = 'percent' # Normalized for error types to show ratio
    
    if not conf_df.empty:
        fig_conf = px.density_heatmap(conf_df, x=x_col, y=y_col, 
                                      text_auto=True, color_continuous_scale="Blues",
                                      histnorm=normalization)
        fig_conf.update_layout(font=font_config, title=title)
        st.plotly_chart(fig_conf, use_container_width=True)
    else:
        st.info("No data available for this view.")

# --------------------------------------------------------
# TAB 5: HIERARCHY (Sunburst & Sankey Flow)
# Purpose: Visualizes the distribution of tests across the hierarchy.
# --------------------------------------------------------
with tab_hier:
    c_h, c_o = st.columns([3, 1])
    with c_h: st.markdown("##### Hierarchical Distribution")
    with c_o: chart_type = st.radio("View As:", ["Sunburst", "Sankey Flow"], horizontal=True)

    if not filtered_df.empty:
        if chart_type == "Sunburst":
            # Standard Sunburst: Cat -> Type -> Rank -> Status
            fig_sun = px.sunburst(
                filtered_df, path=['category', 'type', 'rank', 'status'], 
                color='status', color_discrete_map={"PASS": "#00CC96", "FAIL": "#EF553B"}
            )
            fig_sun.update_layout(font=font_config)
            st.plotly_chart(fig_sun, use_container_width=True)
        else:
            # Dynamic Sankey for Hierarchy: Aggregates counts between levels
            s_labels = []
            label_map = {}
            def get_idx(name):
                if name not in label_map:
                    label_map[name] = len(label_map)
                    s_labels.append(name)
                return label_map[name]

            s_src, s_tgt, s_val, s_col = [], [], [], []
            
            flow_df = filtered_df.groupby(['category', 'type', 'rank', 'status']).size().reset_index(name='count')
            
            for _, row in flow_df.iterrows():
                c = row['count']
                status = row['status']
                l_col = "#98FB98" if status == "PASS" else "#ffcccc" # Link color based on status
                
                i_cat = get_idx(f"Cat: {row['category']}")
                i_typ = get_idx(f"Type: {row['type']}")
                i_rnk = get_idx(f"Rank: {row['rank']}")
                i_sts = get_idx(f"Status: {status}")
                
                # Create Links: Cat->Type, Type->Rank, Rank->Status
                s_src.extend([i_cat, i_typ, i_rnk])
                s_tgt.extend([i_typ, i_rnk, i_sts])
                s_val.extend([c, c, c])
                s_col.extend([l_col, l_col, l_col])
                
            fig_hs = go.Figure(data=[go.Sankey(
                node=dict(pad=15, thickness=20, line=dict(color="black", width=0.5), label=s_labels, color="blue"),
                link=dict(source=s_src, target=s_tgt, value=s_val, color=s_col)
            )])
            fig_hs.update_layout(font=font_config, height=600)
            st.plotly_chart(fig_hs, use_container_width=True)

st.markdown("---")

# ==========================================
# 7. DEEP DIVE INSPECTOR
# ==========================================
st.subheader("üîç Deep Dive Inspector")
# Iterates through filtered results to show raw JSON and errors.
for idx, row in filtered_df.iterrows():
    rec = row['full_record']
    s_icon = "‚úÖ" if row['status'] == "PASS" else "‚ùå"
    
    # Expandable row for each test case
    with st.expander(f"{s_icon} {row['id']} | {row['category']} | {row['actual_tool']}", expanded=False):
        st.code(rec.get('raw_output', ''), language='json')
        st.json(rec.get('errors', {}))
